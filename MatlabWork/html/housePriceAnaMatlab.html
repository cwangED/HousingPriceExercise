
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Chengjia Wang Housing Price Exercise</title><meta name="generator" content="MATLAB 8.6"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-06-20"><meta name="DC.source" content="housePriceAna.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Chengjia Wang Housing Price Exercise</h1><!--introduction--><p><b>This is a regression problem:</b> This project aims to predict the housing prices using UK governments land registry data. Personally, I recommand to use <b>Markov Hidden Model</b> which treat the data as a time-serie for optimal and through solution (housing price is more suitable to solve with consideration of trade time). However, <b>due to the limit of features and time</b> in this exercise, it may make ignorable differences using complicated or mixed prediction models, such as, boosting, Bayesian weighting, etc., or use complicated and state-of-art prediction models such as the ones mentioned in:</p><div><ol><li><a href="http://yann.lecun.com/exdb/publis/pdf/caplin-ssrn-08.pdf">http://yann.lecun.com/exdb/publis/pdf/caplin-ssrn-08.pdf</a></li><li><a href="http://www.hindawi.com/journals/aaa/2014/648047/">http://www.hindawi.com/journals/aaa/2014/648047/</a></li><li><a href="http://www.doc.ic.ac.uk/~mpd37/theses/2015_beng_aaron-ng.pdf">http://www.doc.ic.ac.uk/~mpd37/theses/2015_beng_aaron-ng.pdf</a></li></ol></div><p>Thus in this exercise, I will just use some existing models based on my knowledge directly on the cleaned and sampled data for prediction purposes with little innovation. These simple models include:</p><div><ul><li>KNN and Maximum likelihood Kernel Smooth Regression (MLR)</li><li>Support Vector Regression (SVR)</li><li>Random Forrest (RF)</li></ul></div><p>And the solution is given in (<b>can pick the one you prefer</b>):</p><div><ul><li>Matlab -&gt; LR (Due to best support to matrix algebra and fast prototyping property)</li><li>R -&gt; SVR (Free and clean)</li><li>Python -&gt; RF (Free and wide range of ML lib)</li></ul></div><p><b>Coding style personal option:</b> OOP is not a good way for small-code fast prototyping, but a good dilerable coding style which need careful design</p><p><b>Questions in the exercise description is answered in the code</b></p><p><b>Claim:</b> All the method is implemented by myself, algorithms may refer to Bishop's book and some online source.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Preprocessing: sample and clean data</a></li><li><a href="#2">Initialization</a></li><li><a href="#3">Simplest solution: KNN</a></li><li><a href="#9">Testing analysis Knn</a></li><li><a href="#11">Considering the data as time series</a></li><li><a href="#21">Functions used in Knn Estimater</a></li><li><a href="#22">Kernal based Non-parametric Regression</a></li><li><a href="#27">Using 3rd party packages (PRTools)</a></li><li><a href="#30">Matlab Summary</a></li></ul></div><h2>Preprocessing: sample and clean data<a name="1"></a></h2><div><ul><li>Sampled year prices for quick and easy implementation (e.g., to solve optimization problem using normal equation)</li><li>Encode different types of data all to numeric type</li><li>Details in <i>Preprocessing.m</i> Preprocessing</li></ul></div><h2>Initialization<a name="2"></a></h2><p>set paths, load and rescale data</p><pre class="codeinput">clear; close <span class="string">all</span>; fclose <span class="string">all</span>;

trainStruc.Path = <span class="string">'/home/cwang/Desktop/amazonInterview/trainFile.csv'</span>;
testStruc.Path = <span class="string">'/home/cwang/Desktop/amazonInterview/testFile.csv'</span>;

trainStruc.Data = sortrows(csvread(trainStruc.Path), 2);
testStruc.Data = sortrows(csvread(testStruc.Path), 2);

<span class="comment">% column-wise max-min normalization for data</span>
trainStruc.nomData = mmNormaliz2D(trainStruc.Data);
testStruc.nomData = mmNormaliz2D(testStruc.Data);
<span class="comment">% here the outliers should be found and elimitated</span>
</pre><h2>Simplest solution: KNN<a name="3"></a></h2><p>If time is strictly not allowed to use as a feature, all inputs are discrete variables. Then the simplest naive solution is simply K-nearest neighbour (KNN) and kernal density estimation (KDE).</p><p>Property type is imcomparable qualitative data. Thus the distance function can be defined as logical value</p><pre class="codeinput">trainStruc.X = trainStruc.Data(:, 3:5);
trainStruc.Y = trainStruc.Data(:, 1);

testStruc.X = testStruc.Data(:, 3:5);
testStruc.Y = testStruc.Data(:, 1);

<span class="comment">% define qualitative distance function</span>
knnStruc.distFunc = @(x1, x2) int8(bsxfun(@eq,x1,x2));
</pre><p>1. Dicectly use matlab Knn class (matlab only provided classification function, thus the error will be large)</p><pre class="codeinput">warning <span class="string">off</span>;
<span class="keyword">for</span> nK = 1:20
    knnStruc.md = fitcknn(trainStruc.X, trainStruc.Y, <span class="string">'NumNeighbors'</span>, nK);
    knnStruc.Y = knnStruc.md.predict(testStruc.X);
    knnStruc.error(nK) = sum( sqrt( (knnStruc.Y - testStruc.Y).^2 ) )./<span class="keyword">...</span>
        size(knnStruc.Y, 1);
    knnStruc.mdCross = knnStruc.md.crossval; <span class="comment">% 10 fold cross validation</span>
    disp([num2str(nK), <span class="string">':'</span>]);
    disp([<span class="string">'kfoldloss: '</span>, num2str(knnStruc.mdCross.kfoldLoss)]);
    disp([<span class="string">'LS error: '</span>, num2str(knnStruc.error(nK))]);
<span class="keyword">end</span>
warning <span class="string">on</span>;
disp([<span class="string">'Minmum Matlab Knn RMS error:'</span>, num2str(min(knnStruc.error))])
<span class="comment">% as we can see, treat it as a classification process, extremely bad performance,</span>
<span class="comment">% kfoldloss over 0.99</span>
</pre><p>2. weighted Knn Regression the final prediction is weighted by both the distance and importance of the property (need to implement cross validation and optimization)</p><pre class="codeinput"><span class="comment">% First, decide the K using a simple way. Actually can be decided</span>
<span class="comment">% simultaneously with the weights</span>

tempError = zeros(200, 1);
<span class="keyword">for</span> nK = 1:200 <span class="comment">% candidate Ks</span>
    predFun = @(xTrain, yTrain, xTest) weightedKNNR( xTrain, yTrain, xTest,<span class="keyword">...</span>
        nK);
    tempError(nK) = crossval(<span class="string">'mse'</span>, trainStruc.X, trainStruc.Y, <span class="string">'Predfun'</span>, predFun);

<span class="comment">%         disp(nK)</span>
<span class="keyword">end</span>
</pre><p>find the first smallest error</p><pre class="codeinput">tempInd = 1:size(tempError, 1);
knnStruc.K = min( tempInd( tempError ==  min(tempError) ) );
disp([<span class="string">'optimal Kis: '</span>, num2str(knnStruc.K)]) <span class="comment">% K is 161 in this case, which is a little bit big</span>
</pre><p>but when we plot the errors it shows that when it's bigger than 50, the error doesn't change much. (To find it automatic, use the gradient(x) &gt; thres method.)</p><pre class="codeinput">figure, plot(tempError, <span class="string">'LineWidth'</span>, 3);
snapnow;
knnStruc.K = 50;
knnStruc.IniError = sqrt(tempError(50)); <span class="comment">% it ws mean square error, now RSE</span>
</pre><p>Obtain weitht of features, taking the cross validation error as cost function. To avoid calculation of gradients, we can use particle swarm optimization (PSO), which is a gradient independent global method. I personally invinted a UKF-PSO methods which guide the PSO evolutionary process using unscented Kalman filter. The code will be published online soon after a journal paper is submitted.</p><pre class="codeinput">knnStruc.fw = 1/size(trainStruc.X, 2) .* ones( 1, size(trainStruc.X, 2)-1 );
knnStruc.CostFunc = @(fW) crossErrorKnn(trainStruc.X, trainStruc.Y, fW, knnStruc.K);
knnStruc.optOptions = optimoptions(<span class="string">'particleswarm'</span>,<span class="string">'SwarmSize'</span>,30,<span class="string">'HybridFcn'</span>,@fmincon);
<span class="comment">% knnStruc.optOptions.Display = 'iter';</span>
<span class="comment">% using a trick here to reduce the searching space to 2D</span>
[knnStruc.fw, tempWError] = particleswarm(knnStruc.CostFunc,3, [0 0]', [1 1]', knnStruc.optOptions);
knnStruc.fW = [knnStruc.fw, 1-sum(knnStruc.fw)];
knnStruc.fW = knnStruc.fW./sum(knnStruc.fW);

<span class="comment">% Now the new cross-valid error is:</span>
knnStruc.cvError = sqrt(tempWError);

<span class="comment">% Calculate training error for bias-variance trade-off analysis</span>
knnStruc.trainY = weightedFKNNR(trainStruc.X, trainStruc.Y, trainStruc.X,<span class="keyword">...</span>
    knnStruc.fW, knnStruc.K);

knnStruc.trainError = sqrt(sum((trainStruc.Y - knnStruc.trainY).^2)/size(trainStruc.Y, 1));
</pre><h2>Testing analysis Knn<a name="9"></a></h2><p>finally test the hypothesis "weighted feature KNN Regresser" in weightedFKNNR.m</p><pre class="codeinput">knnStruc.Y = weightedFKNNR(trainStruc.X, trainStruc.Y, testStruc.X,<span class="keyword">...</span>
    knnStruc.fW, knnStruc.K);
knnStruc.testError = sqrt(sum((testStruc.Y - knnStruc.Y).^2)/size(testStruc.Y, 1));

disp(<span class="string">'Feature weighted Knn: '</span>);
disp(<span class="string">'----- Train Result-------'</span>);
disp([num2str(numel(trainStruc.Y)), <span class="string">' training samples: '</span>]);
disp([<span class="string">'Cross Validation Error: '</span>, num2str(knnStruc.cvError)]);
disp([<span class="string">'Training Error: '</span>, num2str(knnStruc.trainError)]);
disp(<span class="string">'----- Test Result -------'</span>);
disp([<span class="string">'Testing Error: '</span>, num2str(knnStruc.testError)]);
</pre><p>As we can see, the</p><h2>Considering the data as time series<a name="11"></a></h2><p>Housing price data is a time series data, which is the main reason real estate become a value maintaining method in many countries. If we can treat the data as time series, a much wider range of algorithm can be used, altough Knn is still applicable (knn is expansive to compute, for saving time, the following code is commented).</p><p>To apply Knn we just need to follow the same procedure shown above with proper initializations</p><div><ul><li>initialization</li></ul></div><pre class="language-matlab"><span class="comment">% normalize time to number of days</span>
testStruc.nomData(:, 2) = trainStruc.nomData(:, 2) - datenum(<span class="string">'1995-1-1'</span>);
testStruc.nomData(:, 2) = trainStruc.nomData(:, 2) - datenum(<span class="string">'1995-1-1'</span>);
</pre><pre class="language-matlab">trainStruc.X = trainStruc.Data(:, 2:5);
trainStruc.Y = trainStruc.Data(:, 1);
testStruc.X = testStruc.Data(:, 3:5);
testStruc.Y = testStruc.Data(:, 1);
</pre><div><ul><li>get K</li></ul></div><pre class="language-matlab">knnStruc.K = 50; <span class="comment">% just being lazy here to save time, should do cross validation</span>
</pre><div><ul><li>Train</li></ul></div><pre class="language-matlab">knnStruc.fW = 1/3 .* ones( 1, size(trainStruc.X, 2) );
knnStruc.CostFunc = @(fW) crossErrorKnn(trainStruc.X, trainStruc.Y, fW, knnStruc.K);
knnStruc.optOptions = optimoptions(<span class="string">'particleswarm'</span>,<span class="string">'SwarmSize'</span>,30,<span class="string">'HybridFcn'</span>,@fmincon);
<span class="comment">%   knnStruc.optOptions.Display = 'iter';</span>
[knnStruc.fW, tempWError] = particleswarm(knnStruc.CostFunc,3, [0 0 0]', [1 1 1]', knnStruc.optOptions);
knnStruc.fW = knnStruc.fW./sum(knnStruc.fW);
knnStruc.trainY = weightedFKNNR(trainStruc.X, trainStruc.Y, trainStruc.X,<span class="keyword">...</span>
              knnStruc.fW, knnStruc.K);
knnStruc.trainError = sqrt(sum((trainStruc.Y - knnStruc.trainY).^2)/size(trainStruc.Y, 1));
</pre><div><ul><li>Test</li></ul></div><pre class="language-matlab">knnStruc.Y = weightedFKNNR(trainStruc.X, trainStruc.Y, testStruc.X,<span class="keyword">...</span>
                knnStruc.fW, knnStruc.K);
knnStruc.testError = sqrt(sum((testStruc.Y - knnStruc.Y).^2)/size(testStruc.Y, 1));
</pre><div><ul><li>Evaluation</li></ul></div><p>We should definitely give</p><div><ol><li>Plots of the testing errors verses different parameter settings.</li><li>Perform <b>Bias-Variance</b> analysis for underfitting and overfitting</li><li>Comparison, Etc.</li></ol></div><h2>Functions used in Knn Estimater<a name="21"></a></h2><div><ul><li>weightedKNNR.m (used to train Knn for optimal K)</li></ul></div><pre class="language-matlab">
<span class="keyword">function</span> outVec = weightedKNNR(matX, matY, inX, K, distFunc)
<span class="comment">% weighted knn regression</span>
<span class="comment">% need a unit test</span>

<span class="comment">% search for k nn</span>
<span class="keyword">if</span> nargin &lt;5
    [indX, DX] = knnsearch(matX, inX, <span class="string">'K'</span>, K);
<span class="keyword">else</span>
    [indX, DX] = knnsearch(matX, inX, <span class="string">'K'</span>, K, <span class="string">'Distance'</span>, distFunc);
<span class="keyword">end</span>

eDX = exp(-DX); <span class="comment">% in case of 0 dist </span>
weights = eDX ./ repmat(sum(eDX, 2), [1, size(eDX, 2)]);
outVec = sum( matY(indX).*weights, 2);

</pre><div><ul><li>crossErrorKnn.m (cross validation error for Knn)</li></ul></div><pre class="language-matlab">
<span class="keyword">function</span> errorOut = crossErrorKnn(trainX, trainY, fw, nK)

<span class="comment">% cross validation error for feature weighted KNN regression</span>
rng(<span class="string">'default'</span>)
fW = [fw 1-sum(fw)];
tempInd  = randi(size(trainY, 1), 1, 100);
predFun = @(xTrain, yTrain, xTest) weightedFKNNR( xTrain, yTrain, xTest, fW./sum(fW), nK);
errorOut = sqrt(crossval(<span class="string">'mse'</span>, trainX(tempInd, :), trainY(tempInd, :), <span class="string">'Predfun'</span>, predFun, <span class="string">'kfold'</span>, 3));

</pre><div><ul><li>weightedFKNNR.m (the final prediction model: feature weighted Knn)</li></ul></div><pre class="language-matlab">
<span class="keyword">function</span> outVec = weightedFKNNR(matX, matY, inX, fW, K)
<span class="comment">% weighted feature knn regression, fW is feature weights</span>
<span class="comment">% need a unit test</span>

<span class="comment">% check feature weights dimension, should be 1XN</span>
assert(size(matX, 2) == size(fW, 2) &amp;&amp; size(fW, 1) == 1);

<span class="comment">% search for k nn</span>
distFunc1 = @(x,Z,wt) sqrt((bsxfun(@minus,x,Z).^2)*wt');
distFunc = @(x1, x2) distFunc1( x1, x2, fW );
[indX, DX] = knnsearch(matX, inX, <span class="string">'K'</span>, K, <span class="string">'Distance'</span>, distFunc);

eDX = exp(-DX); <span class="comment">% in case of 0 dist </span>
weights = eDX ./ repmat(sum(eDX, 2), [1, size(eDX, 2)]);
outVec = sum( matY(indX) .* weights , 2);

</pre><div><ul><li>unit tests</li></ul></div><pre class="language-matlab">
<span class="keyword">classdef</span> knnTests &lt; matlab.unittest.TestCase
    <span class="comment">% Unit Tests for Knn Code (script based is prefered)</span>
    <span class="keyword">properties</span>
    <span class="keyword">end</span>
    
    <span class="keyword">methods</span> (Test)
        <span class="keyword">function</span> testWeightedKNNR(testCase)
            a = magic(100);
            b = 1:100; b = b';
            c = 100:1; c = c';
            K = 20;
            actSize = size( weightedKNNR(a, b, c, K) );
            expSize = size(b);
            testCase.veryfyEqual(actSize, expSize, <span class="string">'ActTol'</span>, 0);
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% No time to finish here... </span>
<span class="comment">% in command line: result = run(knnTest)</span>

</pre><h2>Kernal based Non-parametric Regression<a name="22"></a></h2><p>Again, treating the data as time series, a regression model can be used to solve this problem. Unlike the parametric models which requires to set parameters for kernal basis and regularization weights. Non parametric model can be used, e.g., Nadaraya-Watson kernel regression (simplest one).</p><p>1. reload data and get rid of outliers</p><pre class="codeinput">trainStruc.Path = <span class="string">'/home/cwang/Desktop/amazonInterview/trainFile.csv'</span>;
testStruc.Path = <span class="string">'/home/cwang/Desktop/amazonInterview/testFile.csv'</span>;

trainStruc.Data = sortrows(csvread(trainStruc.Path), 2);
testStruc.Data = sortrows(csvread(testStruc.Path), 2);
trainStruc.Y = trainStruc.Data(:, 1);

<span class="comment">% Outliers elimination:</span>
<span class="comment">% simplified version: get rid of too high prices</span>
<span class="comment">% robust solution should use linear regression + RANSAC (leave for time limit)</span>
outlierInd = (trainStruc.Y &gt;= mean(trainStruc.Y) + 2*std(trainStruc.Y));
trainStruc.Data = trainStruc.Data(outlierInd~=1, :);

<span class="comment">% normalize time to number of days</span>
trainStruc.Data(:, 2) = trainStruc.Data(:, 2) - datenum(<span class="string">'1995-1-1'</span>);
testStruc.Data(:, 2) = testStruc.Data(:, 2) - datenum(<span class="string">'1995-1-1'</span>);

trainStruc.X = trainStruc.Data(:, 2:5);
trainStruc.Y = trainStruc.Data(:, 1);

testStruc.X = testStruc.Data(:, 2:5);
testStruc.Y = testStruc.Data(:, 1);
</pre><p>2. Kernal smooth ML regression (Gaussian Kernel)</p><p>A simple implementation can be found at: Mathwork file exchange. Here's the simplest implementation: fit a time curve in every case (e.g., a curve for property that in london, particular type and lease duration).</p><p>We first try to fit a curve for all train data, later fit a curve for each case using 3rd party tools.</p><pre class="codeinput"><span class="comment">% Fit a GP curve (Matlab internal function):</span>
tic;
MLRStruc.GPModel = fitrgp(trainStruc.X, trainStruc.Y, <span class="string">'KernelFunction'</span>, <span class="keyword">...</span>
    <span class="string">'squaredexponential'</span>);
toc;

<span class="comment">% plot the fitted curve with training data</span>
MLRStruc.GPTrainY = resubPredict(MLRStruc.GPModel);
</pre><pre class="codeinput">figure, plot(trainStruc.X(:, 1), trainStruc.Y, <span class="string">'r*'</span>), hold <span class="string">on</span>;
plot(trainStruc.X(:, 1), MLRStruc.GPTrainY, <span class="string">'b--'</span>, <span class="string">'LineWidth'</span>, 3), hold <span class="string">off</span>;
snapnow;
</pre><p>3. Test:</p><pre class="codeinput"><span class="comment">% make prediction</span>
MLRStruc.GPTestY = predict(MLRStruc.GPModel, testStruc.X);

<span class="comment">% RMS error</span>
MLRStruc.GPError = sqrt(loss(MLRStruc.GPModel, testStruc.X, testStruc.Y));
disp([<span class="string">'Kernel Regression test Error: '</span>, num2str(MLRStruc.GPError)]);
</pre><h2>Using 3rd party packages (PRTools)<a name="27"></a></h2><p><b>PRTools</b> and <b>Vlfeat</b> are widely used Matlab lib which contains popular machine learning and pattern recognition algorithms and tools. Both requires a little bit learning effort, but once mastered this two package, the Matlab code can be much shorter and descent.</p><p>Here we fit a curve for each case (simply linear regression)</p><p>install prtools and import addpath './prtools'</p><pre class="codeinput">addpath(genpath(<span class="string">'./prtools'</span>))
<span class="comment">% addpath(genpath('./prtools4.2.5'))</span>
<span class="comment">% addpath('./@prdataset')</span>



<span class="comment">% encode each cases</span>
PR = [1 2 3 4]; <span class="comment">% property type</span>
LN = [0 1]; <span class="comment">% whether in london</span>
LD = [1 2]; <span class="comment">% lease duration</span>
MLRStruc.codeBook = zeros(4*2*2, 3); <span class="comment">% 4 for property types, 2 for location, 2 for duration</span>
<span class="keyword">for</span> i = 1:4
    <span class="keyword">for</span> j = 1:2
        <span class="keyword">for</span> k = 1:2
            MLRStruc.codeBook((i-1)*4+(j-1)*2 + k, :) = <span class="keyword">...</span>
                [PR(i), LN(j), LD(k)]';
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p>Train: fit a curve for each case: e.g: property type = 1, not in london, lease duration type = 1 The</p><pre class="codeinput">trainStruc.PRDataset = [];
trainStruc.PRRegressor = [];
figure,
<span class="keyword">for</span> nC = 1:size(MLRStruc.codeBook, 1)
    tempInd = (trainStruc.X(:, 2) == MLRStruc.codeBook(nC, 1))<span class="keyword">...</span>
        .* (trainStruc.X(:, 3) == MLRStruc.codeBook(nC, 2))<span class="keyword">...</span>
        .* (trainStruc.X(:, 4) == MLRStruc.codeBook(nC, 3));
    x = trainStruc.X(tempInd==1, 1); <span class="comment">% time serie</span>
    y = trainStruc.Y(tempInd==1);

    <span class="comment">% Make a Prtools training regression dataset</span>
    trainStruc.PRDataset{nC} = gendatr(x, y);

    <span class="comment">% Train a regressor (kernal smooth)</span>
    trainStruc.PRRegressor{nC} = trainStruc.PRDataset{nC}*linearr([], 1);

    <span class="comment">% Plot the regression result</span>
    subplot(4, 4, nC), plot(x, y, <span class="string">'r*'</span>), hold <span class="string">on</span>;
    title([<span class="string">'PR: '</span>, MLRStruc.codeBook(nC, 1), <span class="keyword">...</span>
            <span class="string">', LN: '</span>, MLRStruc.codeBook(nC, 2), <span class="keyword">...</span>
            <span class="string">', LD: '</span>, MLRStruc.codeBook(nC, 3)]);
    plotr(trainStruc.PRRegressor{nC}, <span class="string">'b--'</span>);
    drawnow;
<span class="keyword">end</span>
snapnow;
</pre><p>Test:</p><pre class="codeinput">prwarning <span class="string">off</span>;
testStruc.PRDataset = [];
testStruc.MSE = [];
<span class="keyword">for</span> nC = 1:size(MLRStruc.codeBook, 1)
    tempInd = (testStruc.X(:, 2) == MLRStruc.codeBook(nC, 1))<span class="keyword">...</span>
        .* (testStruc.X(:, 3) == MLRStruc.codeBook(nC, 2))<span class="keyword">...</span>
        .* (testStruc.X(:, 4) == MLRStruc.codeBook(nC, 3));
    x = testStruc.X(tempInd==1, 1); <span class="comment">% time serie</span>
    y = testStruc.Y(tempInd==1);

    <span class="comment">% Make a Prtools test regression dataset</span>
    testStruc.PRDataset{nC} = gendatr(x, y);

    <span class="comment">% Testing the ( mean square error)</span>
    testStruc.MSE{nC} = testStruc.PRDataset{nC}*trainStruc.PRRegressor{nC}*testr;
    <span class="keyword">try</span>
     <span class="comment">% display RMS error</span>
     disp([<span class="string">'Error for PR:'</span>, num2str(MLRStruc.codeBook(nC, 1)), <span class="keyword">...</span>
         <span class="string">', LN:'</span>, num2str(MLRStruc.codeBook(nC, 2)), <span class="string">', LD:'</span>, <span class="keyword">...</span>
         num2str(MLRStruc.codeBook(nC, 3)), <span class="string">': '</span>, num2str(sqrt(testStruc.MSE{nC}))]);
    <span class="keyword">catch</span>
     disp([<span class="string">'Error for PR:'</span>, num2str(MLRStruc.codeBook(nC, 1)), <span class="keyword">...</span>
         <span class="string">', LN:'</span>, num2str(MLRStruc.codeBook(nC, 2)), <span class="string">', LD:'</span>, <span class="keyword">...</span>
         num2str(MLRStruc.codeBook(nC, 3)), <span class="string">': '</span>, <span class="string">'Empty class!!'</span>]);
    <span class="keyword">end</span>
<span class="keyword">end</span>
prwarning <span class="string">on</span>;
</pre><h2>Matlab Summary<a name="30"></a></h2><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Chengjia Wang Housing Price Exercise
%
% *This is a regression problem:*
% This project aims to predict the housing prices using UK governments land
% registry data. Personally, I recommand to use *Markov Hidden Model* which
% treat the data as a time-serie for optimal and through solution (housing
% price is more suitable to solve with consideration of trade time).
% However, *due to the limit of features and time* in this exercise,
% it may make ignorable differences using complicated or mixed prediction models,
% such as, boosting, Bayesian weighting, etc., or
% use complicated and state-of-art prediction models such as the ones mentioned in:
%
%
% # http://yann.lecun.com/exdb/publis/pdf/caplin-ssrn-08.pdf
% # http://www.hindawi.com/journals/aaa/2014/648047/
% # http://www.doc.ic.ac.uk/~mpd37/theses/2015_beng_aaron-ng.pdf
%
% Thus in this exercise, I will just use some existing models based on my
% knowledge directly on the cleaned and sampled data for prediction
% purposes with little innovation. These simple models include:
%
% * KNN and Maximum likelihood Kernel Smooth Regression (MLR)
% * Support Vector Regression (SVR)
% * Random Forrest (RF)
%
% And the solution is given in (*can pick the one you prefer*):
%
%
% * Matlab -> LR (Due to best support to matrix algebra and fast prototyping property)
% * R -> SVR (Free and clean)
% * Python -> RF (Free and wide range of ML lib)
%
% *Coding style personal option:* OOP is not a good way for small-code fast
% prototyping, but a good dilerable coding style which need careful design
%
% *Questions in the exercise description is answered in the code*
%
% *Claim:* All the method is implemented by myself, algorithms may refer to
% Bishop's book and some online source.

%% Preprocessing: sample and clean data
%
%
% * Sampled year prices for quick and easy implementation (e.g., to solve
% optimization problem using normal equation)
% * Encode different types of data all to numeric type
% * Details in _Preprocessing.m_
% Preprocessing

%% Initialization
% set paths, load and rescale data
clear; close all; fclose all;

trainStruc.Path = '/home/cwang/Desktop/amazonInterview/trainFile.csv';
testStruc.Path = '/home/cwang/Desktop/amazonInterview/testFile.csv';

trainStruc.Data = sortrows(csvread(trainStruc.Path), 2);
testStruc.Data = sortrows(csvread(testStruc.Path), 2);

% column-wise max-min normalization for data
trainStruc.nomData = mmNormaliz2D(trainStruc.Data);
testStruc.nomData = mmNormaliz2D(testStruc.Data);
% here the outliers should be found and elimitated

%% Simplest solution: KNN
% If time is strictly not allowed to use as a feature, all inputs are
% discrete variables. Then the simplest naive solution is simply K-nearest
% neighbour (KNN) and kernal density estimation (KDE).
%
% Property type is imcomparable qualitative data. Thus the distance
% function can be defined as logical value

trainStruc.X = trainStruc.Data(:, 3:5);
trainStruc.Y = trainStruc.Data(:, 1);

testStruc.X = testStruc.Data(:, 3:5);
testStruc.Y = testStruc.Data(:, 1);

% define qualitative distance function
knnStruc.distFunc = @(x1, x2) int8(bsxfun(@eq,x1,x2));

%%
% 1. Dicectly use matlab Knn class (matlab only provided classification
% function, thus the error will be large)
warning off;
for nK = 1:20
    knnStruc.md = fitcknn(trainStruc.X, trainStruc.Y, 'NumNeighbors', nK);
    knnStruc.Y = knnStruc.md.predict(testStruc.X);
    knnStruc.error(nK) = sum( sqrt( (knnStruc.Y - testStruc.Y).^2 ) )./...
        size(knnStruc.Y, 1);
    knnStruc.mdCross = knnStruc.md.crossval; % 10 fold cross validation
    disp([num2str(nK), ':']);
    disp(['kfoldloss: ', num2str(knnStruc.mdCross.kfoldLoss)]);
    disp(['LS error: ', num2str(knnStruc.error(nK))]);
end
warning on;
disp(['Minmum Matlab Knn RMS error:', num2str(min(knnStruc.error))])
% as we can see, treat it as a classification process, extremely bad performance,
% kfoldloss over 0.99
%%
% 2. weighted Knn Regression
% the final prediction is weighted by both the distance and importance of
% the property (need to implement cross validation and optimization)

% First, decide the K using a simple way. Actually can be decided
% simultaneously with the weights

tempError = zeros(200, 1);
for nK = 1:200 % candidate Ks
    predFun = @(xTrain, yTrain, xTest) weightedKNNR( xTrain, yTrain, xTest,...
        nK);
    tempError(nK) = crossval('mse', trainStruc.X, trainStruc.Y, 'Predfun', predFun);
    
%         disp(nK)
end

%%
% find the first smallest error
tempInd = 1:size(tempError, 1);
knnStruc.K = min( tempInd( tempError ==  min(tempError) ) );
disp(['optimal Kis: ', num2str(knnStruc.K)]) % K is 161 in this case, which is a little bit big

%%
% but when we plot the errors it shows that when it's bigger than 50, the
% error doesn't change much. (To find it automatic, use the gradient(x) >
% thres method.)
figure, plot(tempError, 'LineWidth', 3);
snapnow;
knnStruc.K = 50;
knnStruc.IniError = sqrt(tempError(50)); % it ws mean square error, now RSE

%%
% Obtain weitht of features, taking the cross validation error as cost
% function. To avoid calculation of gradients, we can use particle swarm
% optimization (PSO), which is a gradient independent global method. I
% personally invinted a UKF-PSO methods which guide the PSO evolutionary
% process using unscented Kalman filter. The code will be published online
% soon after a journal paper is submitted.

knnStruc.fw = 1/size(trainStruc.X, 2) .* ones( 1, size(trainStruc.X, 2)-1 );
knnStruc.CostFunc = @(fW) crossErrorKnn(trainStruc.X, trainStruc.Y, fW, knnStruc.K);
knnStruc.optOptions = optimoptions('particleswarm','SwarmSize',30,'HybridFcn',@fmincon);
% knnStruc.optOptions.Display = 'iter';
% using a trick here to reduce the searching space to 2D
[knnStruc.fw, tempWError] = particleswarm(knnStruc.CostFunc,3, [0 0]', [1 1]', knnStruc.optOptions);
knnStruc.fW = [knnStruc.fw, 1-sum(knnStruc.fw)];
knnStruc.fW = knnStruc.fW./sum(knnStruc.fW);

% Now the new cross-valid error is:
knnStruc.cvError = sqrt(tempWError);

% Calculate training error for bias-variance trade-off analysis
knnStruc.trainY = weightedFKNNR(trainStruc.X, trainStruc.Y, trainStruc.X,...
    knnStruc.fW, knnStruc.K);

knnStruc.trainError = sqrt(sum((trainStruc.Y - knnStruc.trainY).^2)/size(trainStruc.Y, 1));
%% Testing analysis Knn
% finally test the hypothesis "weighted feature KNN Regresser" in
% weightedFKNNR.m
knnStruc.Y = weightedFKNNR(trainStruc.X, trainStruc.Y, testStruc.X,...
    knnStruc.fW, knnStruc.K);
knnStruc.testError = sqrt(sum((testStruc.Y - knnStruc.Y).^2)/size(testStruc.Y, 1));

disp('Feature weighted Knn: ');
disp('REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH- Train ResultREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-');
disp([num2str(numel(trainStruc.Y)), ' training samples: ']);
disp(['Cross Validation Error: ', num2str(knnStruc.cvError)]);
disp(['Training Error: ', num2str(knnStruc.trainError)]);
disp('REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH- Test Result REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-');
disp(['Testing Error: ', num2str(knnStruc.testError)]);

%% 
% As we can see, the 

%% Considering the data as time series
% Housing price data is a time series data, which is the main reason real
% estate become a value maintaining method in many countries. If we can
% treat the data as time series, a much wider range of algorithm can be
% used, altough Knn is still applicable (knn is expansive to compute, for
% saving time, the following code is commented).
%
%
% To apply Knn we just need to follow the same procedure shown above with
% proper initializations
%%
% * initialization
%

%%
%   % normalize time to number of days
%   testStruc.nomData(:, 2) = trainStruc.nomData(:, 2) - datenum('1995-1-1');
%   testStruc.nomData(:, 2) = trainStruc.nomData(:, 2) - datenum('1995-1-1');
%
%   trainStruc.X = trainStruc.Data(:, 2:5);
%   trainStruc.Y = trainStruc.Data(:, 1);
%   testStruc.X = testStruc.Data(:, 3:5);
%   testStruc.Y = testStruc.Data(:, 1);
%%
% * get K
%
%%
%   knnStruc.K = 50; % just being lazy here to save time, should do cross validation

%%
% * Train
%
%%
%   knnStruc.fW = 1/3 .* ones( 1, size(trainStruc.X, 2) );
%   knnStruc.CostFunc = @(fW) crossErrorKnn(trainStruc.X, trainStruc.Y, fW, knnStruc.K);
%   knnStruc.optOptions = optimoptions('particleswarm','SwarmSize',30,'HybridFcn',@fmincon);
% %   knnStruc.optOptions.Display = 'iter';
%   [knnStruc.fW, tempWError] = particleswarm(knnStruc.CostFunc,3, [0 0 0]', [1 1 1]', knnStruc.optOptions);
%   knnStruc.fW = knnStruc.fW./sum(knnStruc.fW);
%   knnStruc.trainY = weightedFKNNR(trainStruc.X, trainStruc.Y, trainStruc.X,...
%                 knnStruc.fW, knnStruc.K);
%   knnStruc.trainError = sqrt(sum((trainStruc.Y - knnStruc.trainY).^2)/size(trainStruc.Y, 1));

%%
% * Test
%
%%
%   knnStruc.Y = weightedFKNNR(trainStruc.X, trainStruc.Y, testStruc.X,...
%                   knnStruc.fW, knnStruc.K);
%   knnStruc.testError = sqrt(sum((testStruc.Y - knnStruc.Y).^2)/size(testStruc.Y, 1));
%%
% * Evaluation
%
% We should definitely give
%
% # Plots of the testing errors verses different parameter settings.
% # Perform *Bias-Variance* analysis for underfitting and overfitting
% # Comparison, Etc.
%

%% Functions used in Knn Estimater
% * weightedKNNR.m (used to train Knn for optimal K)
%
% <include>weightedKNNR.m</include>
%
% * crossErrorKnn.m (cross validation error for Knn)
%
% <include>crossErrorKnn.m</include>
%
% * weightedFKNNR.m (the final prediction model: feature weighted Knn)
%
% <include>weightedFKNNR.m</include>
%
% * unit tests
%
% <include>knnTests.m</include>
%

%% Kernal based Non-parametric Regression
% Again, treating the data as time series, a regression model can be used
% to solve this problem. Unlike the parametric models which requires to set
% parameters for kernal basis and regularization weights. Non parametric
% model can be used, e.g., Nadaraya-Watson kernel regression (simplest one).
%

%%
% 1. reload data and get rid of outliers

trainStruc.Path = '/home/cwang/Desktop/amazonInterview/trainFile.csv';
testStruc.Path = '/home/cwang/Desktop/amazonInterview/testFile.csv';

trainStruc.Data = sortrows(csvread(trainStruc.Path), 2);
testStruc.Data = sortrows(csvread(testStruc.Path), 2);
trainStruc.Y = trainStruc.Data(:, 1);

% Outliers elimination: 
% simplified version: get rid of too high prices
% robust solution should use linear regression + RANSAC (leave for time limit)
outlierInd = (trainStruc.Y >= mean(trainStruc.Y) + 2*std(trainStruc.Y));
trainStruc.Data = trainStruc.Data(outlierInd~=1, :);

% normalize time to number of days
trainStruc.Data(:, 2) = trainStruc.Data(:, 2) - datenum('1995-1-1');
testStruc.Data(:, 2) = testStruc.Data(:, 2) - datenum('1995-1-1');

trainStruc.X = trainStruc.Data(:, 2:5);
trainStruc.Y = trainStruc.Data(:, 1);

testStruc.X = testStruc.Data(:, 2:5);
testStruc.Y = testStruc.Data(:, 1);

%%
% 2. Kernal smooth ML regression (Gaussian Kernel)
%
% A simple implementation can be found at: Mathwork file exchange. Here's
% the simplest implementation: fit a time curve in every case (e.g., a
% curve for property that in london, particular type and lease duration).
%
% We first try to fit a curve for all train data, later fit a curve for
% each case using 3rd party tools.

% Fit a GP curve (Matlab internal function):
tic;
MLRStruc.GPModel = fitrgp(trainStruc.X, trainStruc.Y, 'KernelFunction', ...
    'squaredexponential');
toc;

% plot the fitted curve with training data
MLRStruc.GPTrainY = resubPredict(MLRStruc.GPModel);
%%
figure, plot(trainStruc.X(:, 1), trainStruc.Y, 'r*'), hold on;
plot(trainStruc.X(:, 1), MLRStruc.GPTrainY, 'bREPLACE_WITH_DASH_DASH', 'LineWidth', 3), hold off;
snapnow;

%%
% 3. Test:

% make prediction
MLRStruc.GPTestY = predict(MLRStruc.GPModel, testStruc.X);

% RMS error
MLRStruc.GPError = sqrt(loss(MLRStruc.GPModel, testStruc.X, testStruc.Y));
disp(['Kernel Regression test Error: ', num2str(MLRStruc.GPError)]);

%% Using 3rd party packages (PRTools)
% *PRTools* and *Vlfeat* are widely used Matlab lib which contains popular
% machine learning and pattern recognition algorithms and tools. Both
% requires a little bit learning effort, but once mastered this two
% package, the Matlab code can be much shorter and descent. 
%
% Here we fit a curve for each case (simply linear regression)
%
% install prtools and import
% addpath './prtools'
addpath(genpath('./prtools'))
% addpath(genpath('./prtools4.2.5'))
% addpath('./@prdataset')



% encode each cases
PR = [1 2 3 4]; % property type
LN = [0 1]; % whether in london
LD = [1 2]; % lease duration
MLRStruc.codeBook = zeros(4*2*2, 3); % 4 for property types, 2 for location, 2 for duration
for i = 1:4
    for j = 1:2
        for k = 1:2
            MLRStruc.codeBook((i-1)*4+(j-1)*2 + k, :) = ...
                [PR(i), LN(j), LD(k)]';
        end
    end
end
%%
% Train: fit a curve for each case: e.g: property type = 1, not in london,
% lease duration type = 1
% The 
trainStruc.PRDataset = [];
trainStruc.PRRegressor = [];
figure,
for nC = 1:size(MLRStruc.codeBook, 1)
    tempInd = (trainStruc.X(:, 2) == MLRStruc.codeBook(nC, 1))...
        .* (trainStruc.X(:, 3) == MLRStruc.codeBook(nC, 2))...
        .* (trainStruc.X(:, 4) == MLRStruc.codeBook(nC, 3));
    x = trainStruc.X(tempInd==1, 1); % time serie
    y = trainStruc.Y(tempInd==1);

    % Make a Prtools training regression dataset
    trainStruc.PRDataset{nC} = gendatr(x, y);
    
    % Train a regressor (kernal smooth)
    trainStruc.PRRegressor{nC} = trainStruc.PRDataset{nC}*linearr([], 1);
    
    % Plot the regression result
    subplot(4, 4, nC), plot(x, y, 'r*'), hold on;
    title(['PR: ', MLRStruc.codeBook(nC, 1), ...
            ', LN: ', MLRStruc.codeBook(nC, 2), ...
            ', LD: ', MLRStruc.codeBook(nC, 3)]);
    plotr(trainStruc.PRRegressor{nC}, 'bREPLACE_WITH_DASH_DASH');
    drawnow;
end
snapnow;

%% 
% Test: 
prwarning off;
testStruc.PRDataset = [];
testStruc.MSE = [];
for nC = 1:size(MLRStruc.codeBook, 1)
    tempInd = (testStruc.X(:, 2) == MLRStruc.codeBook(nC, 1))...
        .* (testStruc.X(:, 3) == MLRStruc.codeBook(nC, 2))...
        .* (testStruc.X(:, 4) == MLRStruc.codeBook(nC, 3));
    x = testStruc.X(tempInd==1, 1); % time serie
    y = testStruc.Y(tempInd==1);

    % Make a Prtools test regression dataset
    testStruc.PRDataset{nC} = gendatr(x, y);
    
    % Testing the ( mean square error)
    testStruc.MSE{nC} = testStruc.PRDataset{nC}*trainStruc.PRRegressor{nC}*testr;
    try
     % display RMS error
     disp(['Error for PR:', num2str(MLRStruc.codeBook(nC, 1)), ...
         ', LN:', num2str(MLRStruc.codeBook(nC, 2)), ', LD:', ...
         num2str(MLRStruc.codeBook(nC, 3)), ': ', num2str(sqrt(testStruc.MSE{nC}))]);
    catch
     disp(['Error for PR:', num2str(MLRStruc.codeBook(nC, 1)), ...
         ', LN:', num2str(MLRStruc.codeBook(nC, 2)), ', LD:', ...
         num2str(MLRStruc.codeBook(nC, 3)), ': ', 'Empty class!!']);
    end
end 
prwarning on;
%% Matlab Summary
##### SOURCE END #####
--></body></html>